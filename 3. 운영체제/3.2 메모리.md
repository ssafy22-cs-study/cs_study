# 메모리

## 메모리 계층 = 기억 장치 계층

필요에 따라 (CPU가 메모리에 더 빨리 접근하기 위함) 여러 종류로 나뉘어진다. 이 구조를 바탕으로 컴퓨터의 설계에 있어 상황에 맞게 여러 저장 장치를 각각의 역할이나 특징을 기반으로 사용할 수 있도록 하여 최적의 효율을 낼 수 있다.

![제목 없는 프레젠테이션](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/14f06417-bea9-4169-8f2d-0b707735e287)

### 계층 구조의 목적

> **입출력의 경제성 확보**
> 

전체 기억 장치를 구성하는데 있어서 가격은 최소화하면서 빠른 접근 속도와 대용량의 크기를 제공하기 위해

### 각각에 대해 보기 전에 먼저, CPU 란?

> Central Processing Unit, 중앙 처리 장치
> 
1. 컴퓨터의 4대 주요 기능(기억, 해석, 연산, 제어)을 관할
2. 자체적으로 데이터를 저장할 방법이 없으므로 메모리로 직접 데이터를 전송할 수 없음
    1. 연산을 위해 반드시 레지스터를 거쳐야 함
    2. 이를 위해 레지스터는 특정 주소를 가리키거나 값을 읽어옴

![Untitled](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/792fc8ae-7a89-4830-9d92-2cce97d55108)

### 1. 레지스터(Register)

> CPU 내에서 데이터를 기억하는 메모리 장치
> 
- CPU 내에서 처리할 명령어나 연산에 사용할 값, 연산 결과를 `**일시적(휘발성)**`으로 기억하는 장치
- 메모리 장치 중 속도가 가장 빠르다
- 용량이 가장 적다

### 2. 캐시(Cache)

> 캐시란?
> 

얻고자 하는 데이터를 필요한 순간마다 데이터가 저장되어 있는 저장소에서 가져오는 일에 대한 시간을 줄일 때 사용되는 임시 저장소

= 사용되었던 데이터는 다시 사용되어질 가능성이 높다는 개념에서 시작

= 다시 사용될 확률이 높은 것은 더 빠르게 접근 가능한 저장소를 사용한다

> 목적
> 
- 중앙 처리 장치(CPU)와 주 기억 장치(메인 메모리) 간의 속도 차이 개선
- 전반적인 시스템 성능 개선

= 빠른 장치와 느린 장치에서 속도 차이에 따른 병목 현상을 줄임

= 아무리 CPU 성능이 좋아서 연산 속도가 빨라진다 하더라도 연산에 필요한 데이터가 메인 메모리에서 레지스터로 옮겨질 때까지 기다려야 하는 부분

> L1, L2, L3 로 나뉘어져 있는 이유? = 캐시의 계층 구조
> 
- 보통 L1, L2, L3로 나뉘며 L1부터 CPU에 가깝게 배치
- CPU는 필요한 데이터에 액세스할 때 L1 > L2 > L3 > 주 기억 장치(탐색 후 캐시 저장) > SSD, HDD 순으로 탐색

![Untitled 1](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/97c0d828-1802-488f-9876-ebf11b58d1aa)

> 작동 원리 = **데이터 지역성(자주 사용하는 데이터에 대한 근거)**
> 
- 시간 지역성 (Temporal Locality)
    - 한 번 참조된 데이터는 잠시 후에 또 참조될 가능성이 높다
- 공간 지역성 (Spatial Locality)
    - A[0], A[1] 로 구성되는 배열과 같이 참조된 데이터 근처의 데이터가 잠시후에 사용될 가능성이 높다

> 그럼 L1 = Level 1 캐시?
> 

시스템에 장착된 캐시의 용량과 성능이 점점 증가하면서 **캐시의 캐시**로 사용되는 메모리가 추가

이것이 적용된 순서대로 L(Level) 1, L2, L3… 라고 호칭하는 것

![Untitled 2](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/0f6e35e5-41d1-46cb-b729-2cb99c14315d)

보통 L1 캐시 메모리에는 명령어 캐시와 데이터 캐시가 따로 존재

⇒ 명령어는 보통 `**공간 지역성`** 이 높고 데이터는 보통 `**시간 지역성**`이 높기 때문에, 둘을 나누어 서로 다른 지역성을 이용하도록 함

⇒ 명령어와 데이터를 동시에 읽어올 수 있게 함으로써 CPU 의 파이프라이닝 성능을 향상

> 캐시히트와 캐시미스
> 
- 캐시히트
    - 캐시 메모리에서 원하는 데이터를 찾은 경우
- 캐시미스
    - 찾고자 하는 데이터가 캐시에 없어서 메모리로 가서 데이터를 찾아오는 것
    - 시스템 버스를 기반으로 작동하기 때문에 속도가 느림

> 캐시매핑 = 캐시가 히트되기 위해 매핑하는 방법
> 
1. 직접 매핑 (directed mapping)
    1. 메모리가 1 ~ 100이 있고, 캐시가 1 ~ 10이 있다면 1:1~10, 2:1~20… 식으로 매핑
    2. 처리가 빠르지만 충돌 발생이 잦음
2. 연관 매핑 (associative mapping)
    1. 순서를 일치시키지 않고 관련 있는 캐시와 메모리를 매핑
    2. 충돌이 적지만 모든 블록을 탐색해서 속도가 느림
3. 집합 연관 매핑 (set associative mapping)
    1. 직접 매핑 + 연관 매핑
    2. 순서는 일치시키지만, 집합을 두어 저장하여 블록화되어 있어 검색이 효율적
    3. ex)
        1. 메모리 1 ~ 100 존재, 캐시가 1 ~ 10 존재
        2. 캐시 1 ~ 5에 1 ~ 50의 데이터를 무작위로 저장

> 웹 브라우저에서의 캐시
> 
1. 쿠키
    1. 만료기한이 있는 키-값 저장소
    2. 클라이언트, 서버 설정 가능
2. 로컬 스토리지
    1. 만료기한이 없는 키-값 저장소
    2. 브라우저를 닫아도 유지되고 도메인 단위로 저장, 생성
    3. HTML5를 지원하지 않는 브라우저에서는 사용 불가
    4. 클라이언트에서만 수정 가능
3. 세션 스토리지
    1. 만료기한이 없는 키-값 저장소
    2. 탭 단위로 생성되며, 탭을 닫으면 해당 데이터가 삭제됨
    3. HTML5를 지원하지 않는 브라우저에서는 사용 불가
    4. 클라이언트에서만 수정 가능

> 데이터베이스의 캐싱 계층 = Redis 가 대표적
> 

데이터베이스는 물리 디스크에 직접 데이터를 쓰기 때문에 서버에 문제가 발생해서 다운되더라도 데이터가 손실되지 않음. 

하지만 매번 디스크에 접근해야 하기 때문에 사용자가 많아질수록 부하로 느려질 수 있음

때문에 한 번 읽어온 데이터를 캐시 서버에 저장해서, 첫 번째 요청 이후 저장된 결괏값으로 보여주는 것

### 3. 메인 메모리 (주 기억 장치 = 1차 기억 장치)

실행될 프로그램과 데이터를 기억하며, 여기에 기억된 명령은 한 번에 하나씩 제어 장치에 의해 가져와서 해독된 후 신호로 바뀌어 각 장치로 전달

Q. 어떻게 전달해?

주 기억 장치와 중앙 처리 장치 사이에서 데이터의 이동을 위해 연결된 선들(버스) 를 기반으로 작동

> 구성
> 
- RAM (Random Access Memory)
    - 휘발성 기억 장치
    - 사용자가 요청하는 프로그램이나 문서를 스토리지 디스크에서 메모리로 로드하여 각각의 정보에 액세스
    - 전원이 유지되는 동안 CPU 의 연산 및 동작에 필요한 모든 내용이 저장
    - 전원 종료시 기억된 내용 전부 삭제
    - Random Access
        - 어느 위치에서나 똑같은 속도로 접근하여 읽고 쓰기 가능
    - RAM은 DRAM과 SRAM이 있는데 주기억장치는 주로 DRAM을 의미 (SRAM은 캐시나 레지스터)
- ROM (Read Only Memory)
    - 고정 기억 장치 (비휘발성 영구 저장 메모리)
    - 변경 가능성이 희박한 기능 및 부품에 사용
        - 소프트웨어: 초기 부팅 관련 부분
        - 하드웨어: 프린터 작동에 관여하는 펌웨어 명령 등
    - ROM은 주 기억 장치로 사용되기 보다 주로 기본 입, 출력 시스템, 자가 진단 프로그램 같이 변경 가능성이 희박한 시스템 소프트웨어를 기억시키는데 이용

### 4. 보조 기억 장치 = HDD, SSD, USB 메모리, 광디스크

> 개념
> 
- 하드 디스크, 하드 드라이브, 고정 디스크
- 비휘발성, 순차접근이 가능한 컴퓨터의 보조 기억 장치
- 비휘발성 데이터 저장소 가운데 가장 대중적이며 용량 대비 가격이 가장 저렴

---

## 메모리 관리

운영체제의 대표적인 할 일 중 하나

컴퓨터 내의 한정된 메모리를 극한으로 활용하게 하자!

### 가상 메모리

메모리 관리 기법의 하나로 빠르고 작은 기억 장치(RAM)를 크고 느린 기억 장치(디스크)와 병합하여, 하나의 크고 빠른 기억 장치(가상 메모리)처럼 동작하게 함

⇒ 애플리케이션을 실행하는 데 `**최소한 얼마만큼의 메모리가 필요한가**`에 집중하여 메모리 부족 문제를 해결

- 애플리케이션이 실행될 때, 실행에 필요한 일부분만 메모리에 적재
- 나머지는 디스크에 잔류

> 용어 정리
> 
1. TLB (Translation Lookaside Buffer, 페이지 정보 캐시)
    
    가상 메모리 주소를 물리적 주소로 변환하는 속도를 높이기 위해 사용하는 **캐시**
    
    CPU가 가상 주소를 가지고 메모리에 접근하려고 할 때 우선은 TLB에 접근하여 가상 주소에 해당되는 물리 주소를 찾고, 만약 TLB에 매핑이 존재하지 않는다면 [MMU(Memory Management Unit)](https://ko.wikipedia.org/wiki/%EB%A9%94%EB%AA%A8%EB%A6%AC_%EA%B4%80%EB%A6%AC_%EC%9E%A5%EC%B9%98)가 페이지 테이블에서 해당되는 물리 주소로 변환한 후 메모리에 접근
    
2. 스와핑
    
    메모리에서 당장 사용하지 않는 영역을 하드디스크로 옮기고 하드디스크의 일부분을 마치 메모리처럼 불러와 쓰는 것
    
3. 페이지
    1. 가상 메모리를 사용하는 최소 크기 단위
4. 프레임
    1. 실제 메모리를 사용하는 최소 크기 단위
5. 페이지 폴트
    
    어떤 프로그램이 자신의 주소 공간(가상 메모리 공간)에는 존재하지만 시스템의 RAM에는 현재 존재하지 않는 데이터·코드에 접근을 시도할 경우 발생하는 현상
    
    많이 발생할수록 운영체제의 성능이 저하
    
6. 스레싱
    
    메모리의 페이지 폴트율이 높은 것을 의미
    
    해결 방법>
    
    - 작업 세트
        - 지역성을 통해 결정된 페이지 집합을 만들어서 미리 메모리에 로드하는 것
        - 탐색 비용을 줄이고, 스와핑 또한 줄일 수 있음
    - PFF (Page Fault Frequency)
        - 페이지 폴트 빈도를 조절하는 방법으로 상한선과 하한선을 만듦
        - 상한선에 도달한다면 프레임을 늘리고, 하한선에 도달하면 프레임을 줄임

### 메모리 할당

> 용어 정리
> 
- 내부 단편화 (Internal fragmentation)
    - 메모리를 나눈 크기보다 프로그램이 작아서 들어가지 못하는 공간이 많이 발생하는 현상
    
    ![Untitled 3](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/27099ece-c5ed-45f1-8f59-f5378337aea0)
    
- 외부 단편화 (External fragmentation)
    - 메모리를 나눈 크기보다 프로그램이 커서 들어가지 못하는 (남는) 공간이 많이 발생하는 현상
    
    ![Untitled 4](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/89c7e932-660b-4cfc-a3fb-ce6525044631)

- 홀 (hole)
    - 할당할 수 있는 비어있는 메모리 공간

> 연속 할당
> 

메모리에 ‘연속적으로’ 공간을 할당

1. 고정 분할 방식
    1. 메모리를 미리 나누어 관리하는 방식
    2. 융통성이 없음
    3. 내부 단편화가 발생
2. 가변 분할 방식
    1. 매 시점 프로그램의 크기에 맞게 동적으로 메모리를 나눠 사용
    2. 외부 단편화 발생
    3. 방식
        1. 최초적합
            
            위쪽이나 아래쪽부터 시작해서 홀을 찾으면 바로 할당
            
        2. 최적적합
            
            프로세스의 크기 이상인 공간 중 가장 작은 홀부터 할당
            
        3. 최악적합
            
            프로세스의 크기와 가장 많이 차이가 나는 홀에 할당
            

> 불연속 할당
> 

메모리를 “불연속적으로” 할당

1. 페이징
    1. 동일한 크기의 페이지 단위로 나누어 메모리의 서로 다른 위치에 프로세스를 할당
    2. 홀의 크기가 균일하지 않은 문제 해결
    3. 문제점
        1. 내부 단편화 발생
        2. 주소 변환이 복잡
2. 세그멘테이션
    1. 페이지 단위가 아닌 의미 단위의 세그먼트로 나누는 방식
    2. 코드와 데이터 등을 기반으로 나눌 수도 있고, 함수 단위로 나눌 수 도 있음
    3. 공유와 보안 측면에서 좋음
    4. 문제점
        1. 외부 단편화 발생
        2. 홀 크기가 균일하지 않은 문제 발생
3. 페이지드 세그멘테이션
    1. 공유나 보안을 의미 단위의 세그먼트로 나누고, 물리적 메모리는 페이지로 나누는 것
    2. 세그멘테이션과 페이징의 장점을 취한 방식

### 페이지 교체 알고리즘

메모리는 결국 한정적이므로 스와핑이 일어나지만, 스와핑이 많이 발생하면 성능의 저하로 이어짐

때문에 스와핑이 많이 일어나지 않도록 설계되어야 함

이를 관리하는 기법이 `**페이지 교체 알고리즘**`

- 오프라인 알고리즘 (OPT = Optimal)
    - 먼 미래에 참조되는 페이지와 현재 할당하는 페이지를 바꾸는 알고리즘
    - 가장 이상적인 알고리즘이지만, 미래에 사용될 프로세스를 알 수 없기 때문에 구현 불가
    - 다만 성능 비교에 대한 기준을 제공
- FIFO (First In First Out)
    - 가장 먼저 온 페이지를 교체 영역에 가장 먼저 놓는 방법 (Queue)
- LRU (Least Recently Used)
    - 참조가 가장 오래된 페이지를 교체
    - `오래됨`을 판단하기 위해 각 페이지마다 계수기, 스택을 두어 관리해야 함 ⇒ 오버헤드 발생 가능
    - `시간 지역성(temporal locality)`성질을 고려
    - 가정: 가장 오랫동안 사용하지 않았던 데이터라면 앞으로도 사용할 확률이 적을 것이다
- LFU (Least Frequently Used)
    - 참조 횟수가 가장 작은 페이지 교체
    - LRU는 직전 참조된 시점만을 반영하지만, LFU는 참조횟수를 통해 장기적 시간 규모에서의 참조 성향 고려할 수 있음
    - 단점: 가장 최근에 불러온 페이지가 교체될 수 있음
- MFU (Most Frequently Used)
    - 참조 횟수가 가장 많은 페이지 교체
    - 가정: 가장 많이 사용된 페이지가 앞으로는 사용되지 않을 것이다
- NUR (Not Used Recently, 클럭 알고리즘)
    - 최근에 사용하지 않은 페이지 교체(LRU 에서 발전)
        - 불필요한 공간 낭비 문제를 해결한 알고리즘
    - 교체되는 페이지의 참조 시점이 가장 오래되었다는 것을 보장하지는 못함
    - 각 페이지마다 두 개의 비트 사용
        - 참조(접근) 비트(Reference Bit)
            - 페이지가 참조되지 않았을 때 0, 호출되었을 때 1
            - 읽기, 실행 연산의 경우
            - 모든 참조 비트를 주기적으로 0으로 변경
        - 변형 비트(Modified Bit)
            - 페이지 내용이 변경되지 않았을 때 0, 변경되었을 때 1
            - 쓰기, 추가 연산의 경우 변경
    - 순서
        - 가장 먼저 (0, 0)를 변경할 페이지로 선정
        - 같은 비트의 페이지가 여러 개일 때 맨 위 페이지를 대상 페이지로 선정
        - (0, 1) → (1, 0) → (1, 1) 순서로 선정
        - 모든 페이지가 (1, 1)이 되면 모든 페이지 비트를 (0, 0) 으로 초기화
        
        ![Untitled 5](https://github.com/ssafy22-cs-study/cs_study/assets/52269983/2add7048-9b2f-4ea3-a237-d6d8a23085ad)

